{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "import sys\n",
    "sys.path.append('../../transformers/src')\n",
    "\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import AdamW, pipeline, PegasusForConditionalGeneration, PegasusTokenizer\n",
    "from transformers import BartConfig\n",
    "from transformers import AutoConfig\n",
    "from transformers.models.bart.modeling_bart import EncoderLayer, SinusoidalPositionalEmbedding, LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch_device: cuda\n"
     ]
    }
   ],
   "source": [
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"torch_device:\",torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'google/pegasus-xsum'\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁Yu  ji  ▁Nar  aki  [SAYS]  ▁Hi  ,  ▁John  !  [EOU]  ▁How  ▁are  ▁you  ?  [EOU]  [EOT]  ▁John  [SAYS]  ▁I  '  m  ▁good  .  ▁Thanks  .  [EOU]  [EOT]  ▁Yu  ji  ▁Nar  aki  [SAYS]  ▁Hi  ,  ▁John  !  [EOU]  ▁How  ▁are  ▁you  ?  [EOU]  [EOT]  </s>\n"
     ]
    }
   ],
   "source": [
    "sample_text = [\n",
    "    \"Yuji Naraki [SAYS] Hi, John! [EOU] How are you? [EOU] [EOT] John [SAYS] I'm good. Thanks. [EOU] [EOT] Yuji Naraki [SAYS] Hi, John! [EOU] How are you? [EOU] [EOT]\",\n",
    "    \"Naraki [SAYS] Good evening, Mr.Kim. [EOU] How was your today? [EOU] [EOT] Kim [SAYS] It is a pleasant day. [EOU] [EOT] Daive [SAYS] It is a pleasant day. [EOU] [EOT]\"\n",
    "]\n",
    "special_tokens_dict = {'additional_special_tokens': ['[SAYS]','[EOU]','[EOT]']}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "batch = tokenizer.prepare_seq2seq_batch(sample_text, truncation=True, padding='longest')\n",
    "print('  '.join([tokenizer.convert_ids_to_tokens(i) for i in batch['input_ids'][0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Amanda [SAYS] I baked cookies. [EOU] Do you want some? [EOU] [EOT] Jerry [SAYS] Sure! [EOU] [EOT] Amanda [SAYS] I'll bring you tomorrow :-) [EOU] [EOT]\\n\"]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-9d4bc741f5e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# Operation Check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mtc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTurnConverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspeaker_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meot_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[EOT]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mbatch_speaker_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0membed_speaker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0membed_spk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membed_speaker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_speaker_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-9d4bc741f5e0>\u001b[0m in \u001b[0;36mconvert_batch\u001b[0;34m(self, input_ids)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtext_ids\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mspeaker_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_speaker_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mw_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0;31m# speaker_ids.append(sc.convert_id_to_speaker_id(w_id.item()))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "class TurnConverter():\n",
    "    def __init__(self, speaker_num, eot_idx):\n",
    "        self.speaker_num = speaker_num\n",
    "        self.eot_idx=eot_idx\n",
    "        self.current_speaker_id=1\n",
    "    \n",
    "    def init_speaker_id(self):\n",
    "        self.current_speaker_id=1\n",
    "    \n",
    "    def change_speaker_id(self):\n",
    "        if self.current_speaker_id==1:\n",
    "            self.current_speaker_id = 2\n",
    "        elif self.current_speaker_id==2:\n",
    "            self.current_speaker_id = 1\n",
    "    \n",
    "    def convert_id_to_speaker_id(self, w_id):\n",
    "        if w_id==0:\n",
    "            return 0\n",
    "        elif w_id==self.eot_idx:\n",
    "            self.change_speaker_id()\n",
    "        return self.current_speaker_id\n",
    "\n",
    "    def convert_batch(self, input_ids):\n",
    "        batch_speaker_ids = []\n",
    "        for text_ids in input_ids:\n",
    "            speaker_ids = []\n",
    "            sc.init_speaker_id()\n",
    "            for w_id in text_ids:\n",
    "                # speaker_ids.append(sc.convert_id_to_speaker_id(w_id.item()))\n",
    "                speaker_ids.append(sc.convert_id_to_speaker_id(w_id))\n",
    "            batch_speaker_ids.append(speaker_ids)\n",
    "        return torch.tensor(batch_speaker_ids)\n",
    "\n",
    "# Operation Check\n",
    "tc = TurnConverter(speaker_num = 2, eot_idx = tokenizer.convert_tokens_to_ids('[EOT]'))\n",
    "batch_speaker_ids = tc.convert_batch(batch['input_ids'])\n",
    "embed_speaker = nn.Embedding(3, 10, padding_idx=0)\n",
    "embed_spk = embed_speaker(batch_speaker_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeakerConverter():\n",
    "    def __init__(self, speaker_num, eot_idx):\n",
    "        self.eot_idx=eot_idx\n",
    "        self.current_speaker_id=1\n",
    "        self.speaker_list = []\n",
    "    \n",
    "    def init_speaker_id(self):\n",
    "        self.speaker_list = []\n",
    "        self.current_speaker_id=1\n",
    "    \n",
    "    def change_speaker_id(self):\n",
    "        if self.current_speaker_id==1:\n",
    "            self.current_speaker_id = 2\n",
    "        elif self.current_speaker_id==2:\n",
    "            self.current_speaker_id = 1\n",
    "    \n",
    "    def convert_id_to_speaker_id(self, w_id):\n",
    "        if w_id==0:\n",
    "            return 0\n",
    "        elif w_id==self.eot_idx:\n",
    "            self.change_speaker_id()\n",
    "        return self.current_speaker_id\n",
    "\n",
    "    def convert_batch(self, input_ids):\n",
    "        batch_speaker_ids = []\n",
    "        for text_ids in input_ids:\n",
    "            speaker_ids = []\n",
    "            sc.init_speaker_id()\n",
    "            for w_id in text_ids:\n",
    "                # speaker_ids.append(sc.convert_id_to_speaker_id(w_id.item()))\n",
    "                speaker_ids.append(sc.convert_id_to_speaker_id(w_id))\n",
    "            batch_speaker_ids.append(speaker_ids)\n",
    "        return torch.tensor(batch_speaker_ids)\n",
    "\n",
    "# Operation Check\n",
    "sc = SpeakerConverter(speaker_num = 2, eot_idx = tokenizer.convert_tokens_to_ids('[EOT]'))\n",
    "batch_speaker_ids = sc.convert_batch(batch['input_ids'])\n",
    "embed_speaker = nn.Embedding(99+1, 10, padding_idx=0)\n",
    "embed_spk = embed_speaker(batch_speaker_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "says_token = tokenizer.convert_tokens_to_ids('[SAYS]')\n",
    "eot_token  = tokenizer.convert_tokens_to_ids('[EOT]')\n",
    "pad_token  = tokenizer.convert_tokens_to_ids('<pad>')\n",
    "eod_token  = tokenizer.convert_tokens_to_ids('</s>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeakerConverter():\n",
    "    def __init__(self, says_id, eot_id, eod_id=1, pad_id=0):\n",
    "        self.says_id=says_id\n",
    "        self.eot_idx=eot_id\n",
    "        self.eod_id=eod_id\n",
    "        self.pad_id=pad_id\n",
    "        self.current_speaker_id=1\n",
    "        self.speaker_list = []\n",
    "    \n",
    "    def init_attr(self):\n",
    "        self.current_speaker_id=1\n",
    "        self.speaker_list = []\n",
    "\n",
    "    def get_speaker_id(self, speaker_name):\n",
    "        if speaker_name in self.speaker_list:\n",
    "            return self.speaker_list.index(speaker_name)+1\n",
    "        else:\n",
    "            self.speaker_list.append(speaker_name)\n",
    "            return len(self.speaker_list)\n",
    "        \n",
    "    def change_speaker_id(self, speaker_ids):\n",
    "        if self.eod_id == speaker_ids:\n",
    "            self.current_speaker_id = 0\n",
    "        else:\n",
    "            self.current_speaker_id = self.get_speaker_id('_'.join([str(w_id) for w_id in speaker_ids]))\n",
    "        return self.current_speaker_id\n",
    "    \n",
    "    def convert_batch(self, input_ids):\n",
    "        batch_speaker_ids = []\n",
    "        for text_idx, text_seq in enumerate(input_ids):\n",
    "            speaker_ids = []\n",
    "            self.init_attr()\n",
    "            text_len = len(text_seq)\n",
    "            for w_idx in range(text_len):\n",
    "                if w_idx==0:\n",
    "                    for i in range(w_idx, text_len):\n",
    "                        if self.says_id == text_seq[i]:\n",
    "                            speaker_ids.append(self.current_speaker_id)\n",
    "                            self.change_speaker_id(text_seq[w_idx:i])\n",
    "                            break\n",
    "                elif self.eot_idx == text_seq[w_idx]:\n",
    "                    for i in range(w_idx+1, text_len):\n",
    "                        if self.eod_id == text_seq[i]:\n",
    "                            speaker_ids.append(self.current_speaker_id)\n",
    "                            self.change_speaker_id(self.eod_id)\n",
    "                            break\n",
    "                        elif self.says_id == text_seq[i]:\n",
    "                            speaker_ids.append(self.current_speaker_id)\n",
    "                            self.change_speaker_id(text_seq[w_idx+1:i])\n",
    "                            break\n",
    "                else:\n",
    "                    speaker_ids.append(self.current_speaker_id)\n",
    "            batch_speaker_ids.append(speaker_ids)\n",
    "        return torch.tensor(batch_speaker_ids)\n",
    "\n",
    "# Operation Check\n",
    "sc = SpeakerConverter(\n",
    "    says_id = tokenizer.convert_tokens_to_ids('[SAYS]'),\n",
    "    eot_id = tokenizer.convert_tokens_to_ids('[EOT]'),\n",
    "    eod_id = tokenizer.convert_tokens_to_ids('</s>'),\n",
    "    pad_id = tokenizer.convert_tokens_to_ids('<pad>')\n",
    ")\n",
    "# print(batch['input_ids'])\n",
    "batch_speaker_ids = sc.convert_batch(batch['input_ids'])\n",
    "# print(batch_speaker_ids)\n",
    "embed_speaker = nn.Embedding(99+1, 10, padding_idx=0)\n",
    "embed_spk = embed_speaker(batch_speaker_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 44, 10])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_spk.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96103"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(\"[SAYS]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96105"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(\"[EOT]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁Yu 1\n",
      "ji 1\n",
      "▁Nar 1\n",
      "aki 1\n",
      "[SAYS] 1\n",
      "▁Hi 1\n",
      ", 1\n",
      "▁John 1\n",
      "! 1\n",
      "[EOU] 1\n",
      "▁How 1\n",
      "▁are 1\n",
      "▁you 1\n",
      "? 1\n",
      "[EOU] 1\n",
      "[EOT] 1\n",
      "▁John 2\n",
      "[SAYS] 2\n",
      "▁I 2\n",
      "' 2\n",
      "m 2\n",
      "▁good 2\n",
      ". 2\n",
      "▁Thanks 2\n",
      ". 2\n",
      "[EOU] 2\n",
      "[EOT] 2\n",
      "▁Yu 1\n",
      "ji 1\n",
      "▁Nar 1\n",
      "aki 1\n",
      "[SAYS] 1\n",
      "▁Hi 1\n",
      ", 1\n",
      "▁John 1\n",
      "! 1\n",
      "[EOU] 1\n",
      "▁How 1\n",
      "▁are 1\n",
      "▁you 1\n",
      "? 1\n",
      "[EOU] 1\n",
      "[EOT] 1\n",
      "</s> 0\n",
      "▁Nar 1\n",
      "aki 1\n",
      "[SAYS] 1\n",
      "▁Good 1\n",
      "▁evening 1\n",
      ", 1\n",
      "▁Mr 1\n",
      ". 1\n",
      "Kim 1\n",
      ". 1\n",
      "[EOU] 1\n",
      "▁How 1\n",
      "▁was 1\n",
      "▁your 1\n",
      "▁today 1\n",
      "? 1\n",
      "[EOU] 1\n",
      "[EOT] 1\n",
      "▁Kim 2\n",
      "[SAYS] 2\n",
      "▁It 2\n",
      "▁is 2\n",
      "▁a 2\n",
      "▁pleasant 2\n",
      "▁day 2\n",
      ". 2\n",
      "[EOU] 2\n",
      "[EOT] 2\n",
      "▁Dai 3\n",
      "ve 3\n",
      "[SAYS] 3\n",
      "▁It 3\n",
      "▁is 3\n",
      "▁a 3\n",
      "▁pleasant 3\n",
      "▁day 3\n",
      ". 3\n",
      "[EOU] 3\n",
      "[EOT] 3\n",
      "</s> 0\n",
      "<pad> 0\n",
      "<pad> 0\n",
      "<pad> 0\n",
      "<pad> 0\n"
     ]
    }
   ],
   "source": [
    "for t_i in range(2):\n",
    "    for w_i in range(len(batch_speaker_ids[0])):\n",
    "        print(tokenizer.convert_ids_to_tokens(batch['input_ids'][t_i][w_i]), batch_speaker_ids[t_i][w_i].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t ▁Yuji▁Naraki\n",
      "\t 1\n",
      "▁Yu\n",
      "ji\n",
      "▁Nar\n",
      "aki\n",
      "[SAYS]\n",
      "▁Hi\n",
      ",\n",
      "▁John\n",
      "!\n",
      "[EOU]\n",
      "▁How\n",
      "▁are\n",
      "▁you\n",
      "?\n",
      "[EOU]\n",
      "[EOT]\n",
      "\t ▁John\n",
      "\t 2\n",
      "▁John\n",
      "[SAYS]\n",
      "▁I\n",
      "'\n",
      "m\n",
      "▁good\n",
      ".\n",
      "▁Thanks\n",
      ".\n",
      "[EOU]\n",
      "[EOT]\n",
      "\t end\n",
      "\t 0\n",
      "</s>\n",
      "<pad>\n",
      "\t ▁Naraki\n",
      "\t 1\n",
      "▁Nar\n",
      "aki\n",
      "[SAYS]\n",
      "▁Good\n",
      "▁evening\n",
      ",\n",
      "▁Mr\n",
      ".\n",
      "Kim\n",
      ".\n",
      "[EOU]\n",
      "▁How\n",
      "▁was\n",
      "▁your\n",
      "▁today\n",
      "?\n",
      "[EOU]\n",
      "[EOT]\n",
      "\t ▁Kim\n",
      "\t 2\n",
      "▁Kim\n",
      "[SAYS]\n",
      "▁It\n",
      "▁is\n",
      "▁a\n",
      "▁pleasant\n",
      "▁day\n",
      ".\n",
      "[EOU]\n",
      "[EOT]\n",
      "\t end\n",
      "\t 0\n",
      "</s>\n"
     ]
    }
   ],
   "source": [
    "input_ids = batch['input_ids']\n",
    "for text_idx in range(len(input_ids)):\n",
    "    # init_speaker_list()\n",
    "    speaker_list = []\n",
    "    text_seq = input_ids[text_idx]\n",
    "    for w_idx in range(len(text_seq)):\n",
    "        if w_idx==0:\n",
    "            for i in range(w_idx, len(text_seq)):\n",
    "                if says_token == text_seq[i]:\n",
    "                    print(\"\\t\", ''.join([tokenizer.convert_ids_to_tokens(w_id) for w_id in text_seq[w_idx:i]]))\n",
    "                    print(\"\\t\", get_speaker_id(text_seq[w_idx:i]))\n",
    "                    break\n",
    "        print(tokenizer.convert_ids_to_tokens(text_seq[w_idx]))\n",
    "        if eot_token == text_seq[w_idx]:\n",
    "            for i in range(w_idx+1, len(text_seq)):\n",
    "                # print(i)\n",
    "                if eod_token == text_seq[i]:\n",
    "                    print(\"\\t\", \"end\")\n",
    "                    print(\"\\t\", 0)\n",
    "                    break\n",
    "                elif says_token == text_seq[i]:\n",
    "                    print(\"\\t\", ''.join([tokenizer.convert_ids_to_tokens(w_id) for w_id in text_seq[w_idx+1:i]]))\n",
    "                    print(\"\\t\", get_speaker_id(text_seq[w_idx+1:i]))\n",
    "                    break\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
