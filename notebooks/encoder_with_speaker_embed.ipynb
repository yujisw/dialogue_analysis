{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "import sys\n",
    "# sys.path.append('../../transformers/src')\n",
    "\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import AdamW, pipeline, PegasusForConditionalGeneration, PegasusTokenizer\n",
    "from transformers import BartConfig\n",
    "from transformers import AutoConfig\n",
    "from transformers.models.bart.modeling_bart import EncoderLayer, SinusoidalPositionalEmbedding, LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch_device: cuda\n"
     ]
    }
   ],
   "source": [
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"torch_device:\",torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'google/pegasus-xsum'\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name)\n",
    "# batch = tokenizer.prepare_seq2seq_batch(src_text, truncation=True, padding='longest').to(torch_device)\n",
    "if torch_device == 'cuda':\n",
    "    model = torch.nn.DataParallel(model).to(torch_device)\n",
    "else:\n",
    "    model = model.to(torch_device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad>\n",
      "</s>\n",
      "<unk>\n",
      "<n>\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(tokenizer)):\n",
    "    token = tokenizer.convert_ids_to_tokens(i)\n",
    "    if token.startswith(('<')):\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105\n",
      "106\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_tokens_to_ids('<sep>'))\n",
    "print(tokenizer.convert_tokens_to_ids('<n>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[SAYS]', '[EOU]', '[EOT]']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.additional_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[96103, 96104, 96105]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_tokens = tokenizer.additional_special_tokens\n",
    "[tokenizer.convert_tokens_to_ids(token) for token in special_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "310.0096772683072"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.sqrt(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁Yu  ji  ▁[  SA  YS  ]  ▁Hi  ,  ▁John  !  ▁[  E  OU  ]  ▁How  ▁are  ▁you  ?  ▁[  E  OU  ]  ▁[  E  OT  ]  ▁John  ▁[  say  s  ]  ▁I  '  m  ▁good  .  ▁Thanks  .  ▁[  E  OU  ]  </s>  <pad>  <pad>  <pad>\n",
      "▁Yu  ji  [SAYS]  ▁Hi  ,  ▁John  !  [EOU]  ▁How  ▁are  ▁you  ?  [EOU]  [EOT]  ▁John  ▁[  say  s  ]  ▁I  '  m  ▁good  .  ▁Thanks  .  [EOU]  </s>  <pad>  <pad>  <pad>\n"
     ]
    }
   ],
   "source": [
    "sample_text = [\n",
    "    \"Yuji [SAYS] Hi, John! [EOU] How are you? [EOU] [EOT] John [says] I'm good. Thanks. [EOU]\",\n",
    "    \"Naraki [SAYS] Good evening, Mr.Kim. [EOU] How was your today? [EOU] [EOT] Kim [says] It is a pleasant day. [EOU]\"\n",
    "]\n",
    "batch = tokenizer.prepare_seq2seq_batch(sample_text, truncation=True, padding='longest')\n",
    "print('  '.join([tokenizer.convert_ids_to_tokens(i) for i in batch['input_ids'][0]]))\n",
    "\n",
    "special_tokens_dict = {'additional_special_tokens': ['[SAYS]','[EOU]','[EOT]']}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "model.module.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "batch = tokenizer.prepare_seq2seq_batch(sample_text, truncation=True, padding='longest')\n",
    "print('  '.join([tokenizer.convert_ids_to_tokens(i) for i in batch['input_ids'][0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96105"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids('[EOT]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeakerConverter():\n",
    "    def __init__(self, speaker_num, eot_idx):\n",
    "        self.speaker_num = speaker_num\n",
    "        self.eot_idx=eot_idx\n",
    "        self.current_speaker_id=1\n",
    "    \n",
    "    def init_speaker_id(self):\n",
    "        self.current_speaker_id=1\n",
    "    \n",
    "    def change_speaker_id(self):\n",
    "        if self.current_speaker_id==1:\n",
    "            self.current_speaker_id = 2\n",
    "        elif self.current_speaker_id==2:\n",
    "            self.current_speaker_id = 1\n",
    "    \n",
    "    def convert_id_to_speaker_id(self, w_id):\n",
    "        if w_id==0:\n",
    "            return 0\n",
    "        elif w_id==self.eot_idx:\n",
    "            self.change_speaker_id()\n",
    "        return self.current_speaker_id\n",
    "\n",
    "    def convert_batch(self, input_ids):\n",
    "        batch_speaker_ids = []\n",
    "        for text_ids in input_ids:\n",
    "            speaker_ids = []\n",
    "            sc.init_speaker_id()\n",
    "            for w_id in text_ids:\n",
    "                speaker_ids.append(sc.convert_id_to_speaker_id(w_id.item()))\n",
    "            batch_speaker_ids.append(speaker_ids)\n",
    "        return torch.tensor(batch_speaker_ids)\n",
    "\n",
    "# Operation Check\n",
    "sc = SpeakerConverter(speaker_num = 2, eot_idx = tokenizer.convert_tokens_to_ids('[EOT]'))\n",
    "batch_speaker_ids = sc.convert_batch(batch['input_ids'])\n",
    "embed_speaker = nn.Embedding(3, 10, padding_idx=0)\n",
    "embed_spk = embed_speaker(batch_speaker_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BartEncoderWithSpeakerEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer encoder consisting of *config.encoder_layers* self attention layers. Each layer is a\n",
    "    :class:`EncoderLayer`.\n",
    "    Args:\n",
    "        config: BartConfig\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: BartConfig, embed_tokens):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout = config.dropout\n",
    "        self.layerdrop = config.encoder_layerdrop\n",
    "\n",
    "        embed_dim = embed_tokens.embedding_dim\n",
    "        self.embed_scale = math.sqrt(embed_dim) if config.scale_embedding else 1.0\n",
    "        self.padding_idx = embed_tokens.padding_idx\n",
    "        self.max_source_positions = config.max_position_embeddings\n",
    "\n",
    "        self.embed_tokens = embed_tokens\n",
    "\n",
    "        # speaker embedding setup\n",
    "        self.eot_idx = embed_tokens.num_embeddings - 1\n",
    "        speaker_num = 2 #TODO\n",
    "        self.speaker_converter = SpeakerConverter(speaker_num = speaker_num, eot_idx = self.eot_idx)\n",
    "        self.speaker_embed_scale = 0.1\n",
    "        self.embed_speaker = nn.Embedding(speaker_num+1, embed_dim, padding_idx=0)\n",
    "        \n",
    "        if config.static_position_embeddings:\n",
    "            self.embed_positions = SinusoidalPositionalEmbedding(\n",
    "                config.max_position_embeddings, embed_dim, self.padding_idx\n",
    "            )\n",
    "        else:\n",
    "            self.embed_positions = LearnedPositionalEmbedding(\n",
    "                config.max_position_embeddings,\n",
    "                embed_dim,\n",
    "                self.padding_idx,\n",
    "                config.extra_pos_embeddings,\n",
    "            )\n",
    "        self.layers = nn.ModuleList([EncoderLayer(config) for _ in range(config.encoder_layers)])\n",
    "        self.layernorm_embedding = LayerNorm(embed_dim) if config.normalize_embedding else nn.Identity()\n",
    "        # mbart has one extra layer_norm\n",
    "        self.layer_norm = LayerNorm(config.d_model) if config.add_final_layer_norm else None\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids, attention_mask=None, output_attentions=False, output_hidden_states=False, return_dict=True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_ids (LongTensor): tokens in the source language of shape\n",
    "                `(batch, src_len)`\n",
    "            attention_mask (torch.LongTensor): indicating which indices are padding tokens\n",
    "        Returns:\n",
    "            BaseModelOutput or Tuple comprised of:\n",
    "                - **x** (Tensor): the last encoder layer's output of shape `(src_len, batch, embed_dim)`\n",
    "                - **encoder_states** (tuple(torch.FloatTensor)): all intermediate hidden states of shape `(src_len,\n",
    "                  batch, embed_dim)`. Only populated if *output_hidden_states:* is True.\n",
    "                - **all_attentions** (tuple(torch.FloatTensor)): Attention weights for each layer.\n",
    "                During training might not be of length n_layers because of layer dropout.\n",
    "        \"\"\"\n",
    "        # check attention mask and invert\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = invert_mask(attention_mask)\n",
    "\n",
    "        inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale\n",
    "        embed_pos = self.embed_positions(input_ids)\n",
    "        \n",
    "        batch_speaker_ids = self.speaker_converter.convert_batch(input_ids)\n",
    "        embed_spk = self.embed_speaker(batch_speaker_ids) * self.speaker_embed_scale\n",
    "        \n",
    "        # x = inputs_embeds + embed_pos\n",
    "        x = inputs_embeds + embed_pos + embed_spk\n",
    "        x = self.layernorm_embedding(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        # B x T x C -> T x B x C\n",
    "        x = x.transpose(0, 1)\n",
    "\n",
    "        encoder_states = [] if output_hidden_states else None\n",
    "        all_attentions = () if output_attentions else None\n",
    "        for encoder_layer in self.layers:\n",
    "            if output_hidden_states:\n",
    "                encoder_states.append(x)\n",
    "            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n",
    "            dropout_probability = random.uniform(0, 1)\n",
    "            if self.training and (dropout_probability < self.layerdrop):  # skip the layer\n",
    "                attn = None\n",
    "            else:\n",
    "                x, attn = encoder_layer(x, attention_mask, output_attentions=output_attentions)\n",
    "\n",
    "            if output_attentions:\n",
    "                all_attentions = all_attentions + (attn,)\n",
    "\n",
    "        if self.layer_norm:\n",
    "            x = self.layer_norm(x)\n",
    "        if output_hidden_states:\n",
    "            encoder_states.append(x)\n",
    "            # T x B x C -> B x T x C\n",
    "            encoder_states = tuple(hidden_state.transpose(0, 1) for hidden_state in encoder_states)\n",
    "\n",
    "        # T x B x C -> B x T x C\n",
    "        x = x.transpose(0, 1)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(v for v in [x, encoder_states, all_attentions] if v is not None)\n",
    "        return BaseModelOutput(last_hidden_state=x, hidden_states=encoder_states, attentions=all_attentions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(96106, 1024)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'google/pegasus-xsum'\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name)\n",
    "# batch = tokenizer.prepare_seq2seq_batch(src_text, truncation=True, padding='longest').to(torch_device)\n",
    "# if torch_device == 'cuda':\n",
    "#     model = torch.nn.DataParallel(model).to(torch_device)\n",
    "# else:\n",
    "#     model = model.to(torch_device)\n",
    "\n",
    "special_tokens_dict = {'additional_special_tokens': ['[SAYS]','[EOU]','[EOT]']}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_encoder = model.model.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(\"google/pegasus-xsum\")\n",
    "model.model.encoder = BartEncoderWithSpeakerEmbedding(config, model.model.shared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0048,  0.0194,  0.0144,  ..., -0.0228, -0.0309, -0.0052],\n",
       "        [-0.0075,  0.0200, -0.0099,  ...,  0.0241, -0.0309, -0.0011],\n",
       "        [-0.0077, -0.0009, -0.0259,  ..., -0.0113,  0.0046,  0.0265],\n",
       "        ...,\n",
       "        [ 0.0131,  0.0139,  0.0084,  ...,  0.0091, -0.0060,  0.0289],\n",
       "        [-0.0276, -0.0245, -0.0044,  ..., -0.0249,  0.0062,  0.0243],\n",
       "        [ 0.0029,  0.0141,  0.0160,  ..., -0.0208, -0.0162, -0.0165]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.encoder.layers[0].fc1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.1794, -0.0928,  0.1256,  ..., -0.3336,  0.6347, -0.1645],\n",
       "        [-0.1243,  0.0026, -0.0529,  ..., -0.1189, -0.2767,  0.2300],\n",
       "        [-0.0132, -0.0356,  0.0921,  ...,  0.0906, -0.3452,  0.1085],\n",
       "        ...,\n",
       "        [-0.1842, -0.0378,  0.0911,  ..., -0.4140, -0.3656, -0.1045],\n",
       "        [ 0.1888, -0.0156,  0.0107,  ..., -0.3721,  0.0658,  0.1703],\n",
       "        [-0.4935, -0.0103,  0.1714,  ...,  0.0078, -0.2624,  0.0964]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_encoder.layers[0].fc1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.0190, -0.0234, -0.0032,  ..., -0.0156, -0.0280, -0.0148],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.encoder.layers[0].self_attn.k_proj.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_encoder.layers[0].self_attn.k_proj.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param2 = model.model.encoder.state_dict()\n",
    "for name, param in original_encoder.named_parameters():\n",
    "    param2[name] = original_encoder.state_dict()[name]\n",
    "model.model.encoder.load_state_dict(param2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch_device == 'cuda':\n",
    "    model = torch.nn.DataParallel(model).to(torch_device)\n",
    "else:\n",
    "    model = model.to(torch_device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.module.model.encoder.layers[0].self_attn.k_proj.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 2.8663e-02, -1.9442e-02,  7.4206e-03,  ..., -1.5709e-02,\n",
       "         -1.6828e-02, -2.5521e-02],\n",
       "        [ 1.0093e-02,  2.6218e-02,  2.4884e-02,  ..., -1.2983e-02,\n",
       "         -1.2506e-02, -1.2657e-02],\n",
       "        [-2.9881e-02, -3.0453e-02, -6.0546e-03,  ..., -4.6832e-03,\n",
       "         -1.9767e-02, -9.9810e-03],\n",
       "        ...,\n",
       "        [ 1.7503e-02, -2.7974e-02, -2.9739e-02,  ...,  1.1636e-02,\n",
       "          2.1632e-02,  6.9369e-03],\n",
       "        [-4.0149e-03,  2.1383e-02,  2.8010e-02,  ..., -2.1716e-03,\n",
       "          1.7844e-02,  2.3698e-03],\n",
       "        [-5.2091e-05, -2.3913e-02, -2.1844e-02,  ...,  2.4226e-02,\n",
       "          7.8678e-03, -7.3340e-03]], requires_grad=True)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.module.model.encoder.layers[0].fc1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartEncoderWithSpeakerEmbedding(\n",
       "  (embed_tokens): Embedding(96103, 1024, padding_idx=0)\n",
       "  (embed_speaker): Embedding(3, 1024, padding_idx=0)\n",
       "  (embed_positions): SinusoidalPositionalEmbedding(512, 1024)\n",
       "  (layers): ModuleList(\n",
       "    (0): EncoderLayer(\n",
       "      (self_attn): Attention(\n",
       "        (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "      (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "      (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): EncoderLayer(\n",
       "      (self_attn): Attention(\n",
       "        (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "      (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "      (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): EncoderLayer(\n",
       "      (self_attn): Attention(\n",
       "        (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "      (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "      (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): EncoderLayer(\n",
       "      (self_attn): Attention(\n",
       "        (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "      (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "      (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (4): EncoderLayer(\n",
       "      (self_attn): Attention(\n",
       "        (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "      (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "      (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (5): EncoderLayer(\n",
       "      (self_attn): Attention(\n",
       "        (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "      (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "      (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (6): EncoderLayer(\n",
       "      (self_attn): Attention(\n",
       "        (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "      (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "      (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (7): EncoderLayer(\n",
       "      (self_attn): Attention(\n",
       "        (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "      (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "      (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (8): EncoderLayer(\n",
       "      (self_attn): Attention(\n",
       "        (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "      (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "      (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (9): EncoderLayer(\n",
       "      (self_attn): Attention(\n",
       "        (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "      (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "      (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (10): EncoderLayer(\n",
       "      (self_attn): Attention(\n",
       "        (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "      (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "      (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (11): EncoderLayer(\n",
       "      (self_attn): Attention(\n",
       "        (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "      (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "      (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (12): EncoderLayer(\n",
       "      (self_attn): Attention(\n",
       "        (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "      (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "      (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (13): EncoderLayer(\n",
       "      (self_attn): Attention(\n",
       "        (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "      (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "      (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (14): EncoderLayer(\n",
       "      (self_attn): Attention(\n",
       "        (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "      (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "      (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (15): EncoderLayer(\n",
       "      (self_attn): Attention(\n",
       "        (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "      (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "      (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (layernorm_embedding): Identity()\n",
       "  (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.module.model.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../../Expt_DialogSum/summarizer/args.dat', 'rb') as fp:\n",
    "    args = pickle.load(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "ename": "ContextualVersionConflict",
     "evalue": "(tensorboard 1.15.0 (/home/naraki/.local/lib/python3.6/site-packages), Requirement.parse('tensorboard>=2.2.0'), {'pytorch-lightning'})",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mContextualVersionConflict\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m~/dialogsum/Expt_DialogSum/summarizer/lightning_base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mmin_ver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"1.0.4\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mpkg_resources\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{pkg}>={min_ver}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mpkg_resources\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVersionConflict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mrequire\u001b[0;34m(self, *requirements)\u001b[0m\n\u001b[1;32m    883\u001b[0m         \"\"\"\n\u001b[0;32m--> 884\u001b[0;31m         \u001b[0mneeded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse_requirements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequirements\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mresolve\u001b[0;34m(self, requirements, env, installer, replace_conflicting, extras)\u001b[0m\n\u001b[1;32m    774\u001b[0m                 \u001b[0mdependent_req\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequired_by\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 775\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mVersionConflict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdependent_req\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mContextualVersionConflict\u001b[0m: (tensorboard 1.15.0 (/home/naraki/.local/lib/python3.6/site-packages), Requirement.parse('tensorboard>=2.2.0'), {'pytorch-lightning'})",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mContextualVersionConflict\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-142-fae2151ed797>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../../Expt_DialogSum/summarizer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfinetune\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSummarizationModule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSummarizationModule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSummarizationModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dialogsum/Expt_DialogSum/summarizer/finetune.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0muse_task_specific_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m )\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlightning_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseTransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_generic_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneric_train\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dialogsum/Expt_DialogSum/summarizer/lightning_base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mpkg_resources\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVersionConflict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     logger.warning(\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;34mf\"{pkg}>={min_ver} is required for a normal functioning of this module, but found {pkg}=={pkg_resources.get_distribution(pkg).version}. Try pip install -r examples/requirements.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     )\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mget_distribution\u001b[0;34m(dist)\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRequirement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRequirement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m         \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_provider\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDistribution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expected string, Requirement, or Distribution\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mget_provider\u001b[0;34m(moduleOrReq)\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0;34m\"\"\"Return an IResourceProvider for the named module or requirement\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoduleOrReq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRequirement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mworking_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoduleOrReq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrequire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoduleOrReq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmoduleOrReq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mrequire\u001b[0;34m(self, *requirements)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0mincluded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meven\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mwere\u001b[0m \u001b[0malready\u001b[0m \u001b[0mactivated\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mworking\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m         \"\"\"\n\u001b[0;32m--> 884\u001b[0;31m         \u001b[0mneeded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse_requirements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequirements\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mneeded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mresolve\u001b[0;34m(self, requirements, env, installer, replace_conflicting, extras)\u001b[0m\n\u001b[1;32m    773\u001b[0m                 \u001b[0;31m# Oops, the \"best\" so far conflicts with a dependency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m                 \u001b[0mdependent_req\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequired_by\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 775\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mVersionConflict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdependent_req\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# push the new requirements onto the stack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mContextualVersionConflict\u001b[0m: (tensorboard 1.15.0 (/home/naraki/.local/lib/python3.6/site-packages), Requirement.parse('tensorboard>=2.2.0'), {'pytorch-lightning'})"
     ]
    }
   ],
   "source": [
    "sys.path.append('../../Expt_DialogSum/summarizer')\n",
    "from finetune import SummarizationModule\n",
    "model: SummarizationModule = SummarizationModule(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_text = [\n",
    "    \"\"\" PG&E stated it scheduled the blackouts in response to forecasts for high winds amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.\"\"\"\n",
    "]\n",
    "batch = tokenizer.prepare_seq2seq_batch(src_text, truncation=True, padding='longest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = batch.to(torch_device)\n",
    "translated = model.module.generate(**batch)\n",
    "tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "assert tgt_text[0] == \"California's largest electricity provider has turned off power to hundreds of thousands of customers.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>summary</th>\n",
       "      <th>dialogue</th>\n",
       "      <th>speaker_num</th>\n",
       "      <th>dialogue_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13818513</td>\n",
       "      <td>Amanda baked cookies and will bring Jerry some...</td>\n",
       "      <td>Amanda: I baked  cookies. Do you want some?\\r\\...</td>\n",
       "      <td>2</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13728867</td>\n",
       "      <td>Olivia and Olivier are voting for liberals in ...</td>\n",
       "      <td>Olivia: Who are you voting for in this electio...</td>\n",
       "      <td>2</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13681000</td>\n",
       "      <td>Kim may try the pomodoro technique recommended...</td>\n",
       "      <td>Tim: Hi, what's up?\\r\\nKim: Bad mood tbh, I wa...</td>\n",
       "      <td>2</td>\n",
       "      <td>528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13730747</td>\n",
       "      <td>Edward thinks he is in love with Bella. Rachel...</td>\n",
       "      <td>Edward: Rachel, I think I'm in ove with Bella....</td>\n",
       "      <td>2</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13728094</td>\n",
       "      <td>Sam is confused, because he overheard Rick com...</td>\n",
       "      <td>Sam: hey  overheard rick say something\\r\\nSam:...</td>\n",
       "      <td>2</td>\n",
       "      <td>909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                            summary  \\\n",
       "0  13818513  Amanda baked cookies and will bring Jerry some...   \n",
       "1  13728867  Olivia and Olivier are voting for liberals in ...   \n",
       "2  13681000  Kim may try the pomodoro technique recommended...   \n",
       "3  13730747  Edward thinks he is in love with Bella. Rachel...   \n",
       "4  13728094  Sam is confused, because he overheard Rick com...   \n",
       "\n",
       "                                            dialogue  speaker_num  \\\n",
       "0  Amanda: I baked  cookies. Do you want some?\\r\\...            2   \n",
       "1  Olivia: Who are you voting for in this electio...            2   \n",
       "2  Tim: Hi, what's up?\\r\\nKim: Bad mood tbh, I wa...            2   \n",
       "3  Edward: Rachel, I think I'm in ove with Bella....            2   \n",
       "4  Sam: hey  overheard rick say something\\r\\nSam:...            2   \n",
       "\n",
       "   dialogue_len  \n",
       "0            94  \n",
       "1           111  \n",
       "2           528  \n",
       "3           155  \n",
       "4           909  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_dir = \"/home/naraki/dialogsum/corpus\"\n",
    "df_train = pd.read_table(os.path.join(corpus_dir,\"train.tsv\"), index_col=0)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogues = list(df_train['dialogue'][:4].values)\n",
    "summaries = list(df_train['summary'][:4].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Jerry: Hello, Amanda.',\n",
       " \"Olivia: Hi, I'm Olivia from Newsround and I'm here to answer your questions.\",\n",
       " \"Kim: Hi Tim, what's up?\",\n",
       " \"Rachel: I'm outside.\"]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = tokenizer.prepare_seq2seq_batch(dialogues, truncation=True, padding='longest').to(torch_device)\n",
    "# batch = tokenizer.prepare_seq2seq_batch(dialogues, truncation=True, max_length=256).to(torch_device)\n",
    "translated = model.module.generate(**batch)\n",
    "tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "tgt_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "# no_decay = ['bias', 'LayerNorm.weight']\n",
    "# optimizer_grouped_parameters = [\n",
    "#     {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "#     {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "# ]\n",
    "# optimizer = AdamW(optimizer_grouped_parameters, lr=1e-5)\n",
    "encoding = tokenizer(dialogues, return_tensors='pt', padding=True, truncation=True)\n",
    "# input_ids = encoding['input_ids'].to(torch_device)\n",
    "# attention_mask = encoding['attention_mask'].to(torch_device)\n",
    "input_ids = encoding['input_ids']\n",
    "attention_mask = encoding['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(input_ids, attention_mask=attention_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 131, 96103])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'logits'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-62ca31ea085e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'logits'"
     ]
    }
   ],
   "source": [
    "outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-d555cff3c3a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'loss'"
     ]
    }
   ],
   "source": [
    "loss = outputs.loss\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.0000e+00,  1.2714e+01,  6.7395e-01,  ..., -3.6678e+00,\n",
       "            2.4565e-01, -3.3426e+00],\n",
       "          [ 0.0000e+00,  1.3197e+01,  7.1254e-01,  ..., -4.1114e+00,\n",
       "           -1.5395e+00, -1.0908e+00],\n",
       "          [ 0.0000e+00,  1.3785e+01,  6.9809e-01,  ..., -6.1680e+00,\n",
       "           -1.4458e+00, -1.1608e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  1.3297e+01,  8.8518e-02,  ..., -4.8198e+00,\n",
       "           -3.4773e-01, -9.2595e-01],\n",
       "          [ 0.0000e+00,  1.2996e+01,  3.6114e-01,  ..., -4.4285e+00,\n",
       "            1.8546e+00, -2.1107e+00],\n",
       "          [ 0.0000e+00,  1.3119e+01,  7.2225e-01,  ..., -3.4602e+00,\n",
       "            1.8937e+00, -6.2107e+00]],\n",
       " \n",
       "         [[ 0.0000e+00,  1.3079e+01,  1.3642e+00,  ..., -3.3401e+00,\n",
       "           -2.5243e+00, -4.6213e+00],\n",
       "          [ 0.0000e+00,  1.0686e+01,  3.9215e-01,  ..., -2.0992e+00,\n",
       "           -1.0811e+00, -5.1900e+00],\n",
       "          [ 0.0000e+00,  1.0144e+01,  1.0483e+00,  ..., -1.7541e+00,\n",
       "           -1.7547e+00, -3.3855e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  1.4253e+01,  2.7133e-01,  ...,  1.7963e+00,\n",
       "            1.2237e+00, -1.5540e+00],\n",
       "          [ 0.0000e+00,  1.4063e+01, -8.6137e-02,  ...,  1.4144e+00,\n",
       "            1.4896e+00, -1.9142e+00],\n",
       "          [ 0.0000e+00,  1.2085e+01,  4.5689e-01,  ..., -1.0841e+00,\n",
       "            7.2731e-01,  5.9480e-02]],\n",
       " \n",
       "         [[ 0.0000e+00,  9.0624e+00,  1.6778e-01,  ..., -6.4382e+00,\n",
       "            2.8920e-01, -4.7970e+00],\n",
       "          [ 0.0000e+00,  1.1109e+01,  6.5121e-01,  ..., -4.6718e+00,\n",
       "           -4.5153e+00, -1.0573e+00],\n",
       "          [ 0.0000e+00,  1.0215e+01,  3.7343e-01,  ..., -6.4871e+00,\n",
       "           -1.6648e+00,  2.0164e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  1.0925e+01,  6.4523e-01,  ..., -3.2136e+00,\n",
       "           -9.0205e-01,  1.3920e+00],\n",
       "          [ 0.0000e+00,  1.3565e+01,  1.8836e-01,  ..., -1.4671e+00,\n",
       "           -1.9649e+00,  6.9764e-01],\n",
       "          [ 0.0000e+00,  1.3220e+01, -1.9044e-01,  ..., -2.2429e+00,\n",
       "           -1.0686e+00,  3.2442e+00]],\n",
       " \n",
       "         [[ 0.0000e+00,  1.3680e+01,  8.4229e-01,  ..., -3.1720e+00,\n",
       "           -3.6875e-03, -4.0502e+00],\n",
       "          [ 0.0000e+00,  1.5273e+01,  3.4214e-01,  ..., -3.5735e+00,\n",
       "           -1.0787e+00, -7.2024e+00],\n",
       "          [ 0.0000e+00,  1.2216e+01,  4.5859e-01,  ..., -9.5311e-01,\n",
       "            9.7280e-01, -2.8009e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  1.2838e+01,  5.1605e-01,  ..., -7.6881e-01,\n",
       "           -8.0520e-01, -4.0723e+00],\n",
       "          [ 0.0000e+00,  1.5293e+01,  3.1126e-01,  ..., -2.3286e+00,\n",
       "           -2.1227e+00, -4.0008e+00],\n",
       "          [ 0.0000e+00,  9.9458e+00,  4.1473e-01,  ..., -1.6551e-01,\n",
       "           -2.2050e+00, -3.8992e+00]]], device='cuda:0',\n",
       "        grad_fn=<GatherBackward>),\n",
       " tensor([[[ 9.8554e-02,  2.0136e-01, -8.7661e-03,  ..., -5.2155e-02,\n",
       "           -8.0756e-02, -1.2259e-01],\n",
       "          [-3.1981e-02,  8.1200e-04, -1.6915e-01,  ..., -7.7707e-02,\n",
       "            2.7665e-01, -3.6566e-01],\n",
       "          [ 4.2205e-02,  1.8075e-02,  2.7361e-02,  ..., -3.1643e-02,\n",
       "            5.8098e-02, -9.6294e-02],\n",
       "          ...,\n",
       "          [ 1.1579e-02, -2.5810e-02,  1.2633e-01,  ...,  1.5468e-01,\n",
       "            3.5464e-02,  6.9020e-04],\n",
       "          [ 3.9609e-02, -4.5270e-02,  1.1110e-01,  ..., -4.4396e-03,\n",
       "           -3.5147e-02, -6.6195e-02],\n",
       "          [ 7.0570e-03, -5.7442e-03,  8.0430e-02,  ..., -3.9546e-04,\n",
       "           -3.2738e-02,  5.2731e-02]],\n",
       " \n",
       "         [[ 8.4701e-02,  3.2646e-02,  4.6448e-02,  ..., -1.3393e-01,\n",
       "            6.6628e-03, -7.3716e-02],\n",
       "          [ 3.3287e-02, -1.2202e-01, -6.3385e-02,  ..., -3.8932e-01,\n",
       "            3.0007e-01, -4.0564e-01],\n",
       "          [-1.8268e-01, -1.6196e-01, -3.2054e-03,  ..., -1.5523e-01,\n",
       "            1.9129e-02, -1.1558e-01],\n",
       "          ...,\n",
       "          [ 6.5009e-02, -1.2755e-01,  1.2839e-01,  ...,  1.0148e-01,\n",
       "            2.0248e-01,  4.4134e-02],\n",
       "          [ 2.7336e-02, -3.4008e-02,  1.3790e-01,  ...,  1.0811e-01,\n",
       "            6.5745e-02, -9.7859e-02],\n",
       "          [ 2.9469e-02, -1.6369e-01,  5.7574e-02,  ...,  7.8618e-02,\n",
       "            2.8593e-01, -5.8486e-02]],\n",
       " \n",
       "         [[ 2.4196e-02,  3.9674e-02, -9.7502e-02,  ...,  2.6074e-02,\n",
       "           -2.6486e-01, -1.2020e-01],\n",
       "          [ 1.4875e-01, -6.1090e-02, -3.4612e-01,  ..., -2.2627e-01,\n",
       "            3.3302e-01, -3.2747e-01],\n",
       "          [ 1.1382e-01, -3.4581e-02, -7.5311e-02,  ..., -2.0597e-01,\n",
       "            2.7274e-01, -5.8400e-02],\n",
       "          ...,\n",
       "          [ 1.4241e-01,  1.4713e-01,  2.5837e-02,  ...,  1.6720e-01,\n",
       "           -2.6063e-02, -2.4950e-01],\n",
       "          [ 3.3806e-01,  7.4022e-02, -1.0191e-01,  ...,  2.3902e-01,\n",
       "           -1.6416e-01,  1.3275e-01],\n",
       "          [ 1.4051e-01, -5.0661e-02, -1.2140e-01,  ...,  1.9845e-01,\n",
       "           -8.7014e-02,  4.7121e-02]],\n",
       " \n",
       "         [[ 1.4585e-01,  6.0273e-02,  4.6999e-02,  ...,  1.4126e-02,\n",
       "            6.1185e-02,  6.4320e-02],\n",
       "          [ 5.1965e-02, -1.7392e-01, -1.1155e-01,  ..., -3.7686e-02,\n",
       "            3.7966e-01, -3.0764e-02],\n",
       "          [ 6.0957e-02,  6.2163e-02, -3.0586e-03,  ..., -2.1705e-02,\n",
       "            1.0598e-01, -1.1994e-02],\n",
       "          ...,\n",
       "          [ 5.4035e-03, -3.6137e-03,  3.0114e-02,  ..., -3.2636e-03,\n",
       "           -1.1660e-02, -5.2553e-03],\n",
       "          [-1.0409e-02, -7.2038e-03, -7.5761e-02,  ...,  3.8670e-03,\n",
       "            1.5004e-03,  2.2436e-02],\n",
       "          [-5.3695e-02,  9.8668e-02,  6.7057e-02,  ..., -5.2567e-02,\n",
       "            6.7892e-02,  1.7630e-02]]], device='cuda:0',\n",
       "        grad_fn=<GatherBackward>))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
